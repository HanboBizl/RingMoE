# cloud context init config
seed: 0
context:
  mode: 0 #0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  enable_graph_kernel: False
  graph_kernel_flags: "--opt_level=0"
  max_call_depth: 10000
  save_graphs: False
  device_id: 0

use_parallel: True
parallel:
  parallel_mode: 1 # 0-data, 1-semi, 2-auto, 3-hybrid
  gradients_mean: False
  enable_alltoall: True  # moe context
  full_batch: False  # semi --> True
  loss_repeated_mean: True
  search_mode: "sharding_propagation"  # auto mode is valid
  enable_parallel_optimizer: True  # 优化器并行
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"

# context config for transformer
moe_config: # 参考 mindspore.nn.transformer.moe
  expert_num: 4  #  专家数量  examples: 32
  capacity_factor: 1.2  # token容量 候选token比例
  aux_loss_factor: 0.001  # moe loss 占比
  num_experts_chosen: 1  # token choose experts num

recompute_config: # recompute
  recompute: True  #  default
  parallel_optimizer_comm_recompute: False  #
  mp_comm_recompute: True
  recompute_slice_activation: False

parallel_config:
  data_parallel: 32 # dp * mp <= total cards num 2机16卡 16
  model_parallel: 1
  expert_parallel: 8
  pipeline_stage: 1
  optimizer_shard: True
  micro_batch_num: 1
  gradient_aggregation_group: 4